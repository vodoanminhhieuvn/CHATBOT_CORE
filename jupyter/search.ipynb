{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_dict = [\n",
    "    { \"name\":\"John\", \"age\":30, \"city\":\"New York\"},\n",
    "    { \"name\":\"hello\", \"age\":30, \"city\":\"New York\"},\n",
    "    { \"name\":\"Cơm gà\", \"age\":30, \"city\":\"New York\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existed\n"
     ]
    }
   ],
   "source": [
    "for food in food_dict:\n",
    "    if \"Cơm gà\" in food[\"name\"]:\n",
    "        print(\"Existed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = u'ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚÝàáâãèéêìíòóôõùúýĂăĐđĨĩŨũƠơƯưẠạẢảẤấẦầẨẩẪẫẬậẮắẰằẲẳẴẵẶặẸẹẺẻẼẽẾếỀềỂểỄễỆệỈỉỊịỌọỎỏỐốỒồỔổỖỗỘộỚớỜờỞởỠỡỢợỤụỦủỨứỪừỬửỮữỰựỲỳỴỵỶỷỸỹ'\n",
    "s0 = u'AAAAEEEIIOOOOUUYaaaaeeeiioooouuyAaDdIiUuOoUuAaAaAaAaAaAaAaAaAaAaAaAaEeEeEeEeEeEeEeEeIiIiOoOoOoOoOoOoOoOoOoOoOoOoUuUuUuUuUuUuUuYyYyYyYy'\n",
    "def remove_accents(input_str):\n",
    "\ts = ''\n",
    "\tfor c in input_str:\n",
    "\t\tif c in s1:\n",
    "\t\t\ts += s0[s1.index(c)]\n",
    "\t\telse:\n",
    "\t\t\ts += c\n",
    "\treturn s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ascii(text):\n",
    "    ascii_values = [ord(character) for character in text]\n",
    "    return ascii_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def remove_ascii_accent(list_ascii: List[int]):\n",
    "    for code in list_ascii:\n",
    "        if code >= 768 and code <= 879:\n",
    "            list_ascii.remove(code)\n",
    "        \n",
    "    return list_ascii\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 114, 117, 110, 103]\n",
      "[84, 114, 117, 110, 103, 32, 67, 104, 105, 101, 110, 32, 82, 105, 109, 32, 78, 117, 111, 99, 32, 77, 97, 109]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "\n",
    "print(remove_ascii_accent(to_ascii(remove_accents('Trứng'))))\n",
    "print(remove_ascii_accent(to_ascii(remove_accents('Trứng Chiên Rim Nước Mắm'))))\n",
    "\n",
    "needed_ascii_word = remove_ascii_accent(to_ascii(remove_accents('Trứng')))\n",
    "ascii_word = remove_ascii_accent(to_ascii(remove_accents('Trứng Chiên Rim Nước Mắm')))\n",
    "\n",
    "if set(needed_ascii_word).issubset(set(ascii_word)):\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")\n",
    "\n",
    "\n",
    "# filtered_word = ''.join(map(chr, ascii_word))\n",
    "\n",
    "# 768-879\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mình', 'muốn', 'rim', 'nước mắm']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/vodoanminhhieu/Development/chatbot_core/jupyter/search.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vodoanminhhieu/Development/chatbot_core/jupyter/search.ipynb#ch0000002?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m food_token \u001b[39min\u001b[39;00m food_rating_token:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vodoanminhhieu/Development/chatbot_core/jupyter/search.ipynb#ch0000002?line=30'>31</a>\u001b[0m     food_rating_score\u001b[39m.\u001b[39mappend(\u001b[39mlen\u001b[39m(food_token))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vodoanminhhieu/Development/chatbot_core/jupyter/search.ipynb#ch0000002?line=32'>33</a>\u001b[0m max_score \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39;49m(food_rating_score)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vodoanminhhieu/Development/chatbot_core/jupyter/search.ipynb#ch0000002?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(food_rating_score)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vodoanminhhieu/Development/chatbot_core/jupyter/search.ipynb#ch0000002?line=35'>36</a>\u001b[0m list_highest_score \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i, j \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(food_rating_score) \u001b[39mif\u001b[39;00m j \u001b[39m==\u001b[39m max_score]\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "from numpy import append\n",
    "from underthesea import word_tokenize\n",
    "import jellyfish\n",
    "\n",
    "user_message = \"mình muốn rim nước mắm\".lower()\n",
    "\n",
    "user_message_tokenizer = word_tokenize(user_message)\n",
    "\n",
    "food_respones = [\n",
    "    {\"name\": \"Trứng Chiên rim sa tế\"},\n",
    "    {\"name\": \"Trứng Chiên rim nước mắm\"},\n",
    "]\n",
    "\n",
    "food_rating_token = []\n",
    "\n",
    "print(user_message_tokenizer)\n",
    "\n",
    "for index, food in enumerate(food_respones):\n",
    "    \n",
    "    for word in user_message_tokenizer:\n",
    "        if word.lower() in food[\"name\"].lower():\n",
    "            try :\n",
    "                if word.lower() not in food_rating_token[index]:\n",
    "                    food_rating_token[index].append(word)\n",
    "            except IndexError:\n",
    "                food_rating_token.append([word])\n",
    "\n",
    "food_rating_score = []\n",
    "\n",
    "for food_token in food_rating_token:\n",
    "    food_rating_score.append(len(food_token))\n",
    "\n",
    "max_score = max(food_rating_score)\n",
    "print(food_rating_score)\n",
    "\n",
    "list_highest_score = [i for i, j in enumerate(food_rating_score) if j == max_score]\n",
    "\n",
    "if len(list_highest_score) > 1:\n",
    "    print(\"Recipes\")\n",
    "    for index in list_highest_score:\n",
    "        print(food_respones[index][\"name\"])\n",
    "\n",
    "else:\n",
    "    print(\"Here your recipe\")\n",
    "    print(food_respones[index][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5972222222222222\n",
      "0.5873015873015873\n",
      "------------\n",
      "0.5972222222222222\n",
      "0.5873015873015873\n",
      "------------\n",
      "13\n",
      "18\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import jellyfish\n",
    "from ngramratio import ngramratio\n",
    "SequenceMatcherExtended = ngramratio.SequenceMatcherExtended\n",
    "import difflib\n",
    "\n",
    "print(jellyfish.jaro_distance(u'Gà Chiên Mắm Tỏi'.lower(), u'Gỏi'.lower()))\n",
    "print(jellyfish.jaro_distance(u'Chân Gà Chiên Mắm Tỏi'.lower(), u'Gỏi'.lower()))\n",
    "\n",
    "\n",
    "print('------------')\n",
    "\n",
    "print(jellyfish.jaro_winkler_similarity(u'Gà Chiên Mắm Tỏi'.lower(), u'Gỏi'.lower()))\n",
    "print(jellyfish.jaro_winkler_similarity(u'Chân Gà Chiên Mắm Tỏi'.lower(), u'Gỏi'.lower()))\n",
    "\n",
    "print('------------')\n",
    "\n",
    "print(jellyfish.damerau_levenshtein_distance(u'Gà Chiên Mắm Tỏi'.lower(), u'Tỏi'.lower()))\n",
    "print(jellyfish.damerau_levenshtein_distance(u'Chân Gà Chiên Mắm Tỏi'.lower(), u'Tỏi'.lower()))\n",
    "\n",
    "seq=difflib.SequenceMatcher(u'Gà Chiên Mắm Tỏi'.lower(), u'Gà'.lower())\n",
    "print(seq.ratio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked\n"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "words = word_tokenize(\"Hello GÀ\")\n",
    "\n",
    "\n",
    "if any(word.lower() in \"gà machiato\".lower() for word in words):\n",
    "    print('Checked');\n",
    "else:\n",
    "    print(\"Not Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCENT_STRING = u'ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚÝàáâãèéêìíòóôõùúýĂăĐđĨĩŨũƠơƯưẠạẢảẤấẦầẨẩẪẫẬậẮắẰằẲẳẴẵẶặẸẹẺẻẼẽẾếỀềỂểỄễỆệỈỉỊịỌọỎỏỐốỒồỔổỖỗỘộỚớỜờỞởỠỡỢợỤụỦủỨứỪừỬửỮữỰựỲỳỴỵỶỷỸỹ'\n",
    "NON_ACCENT_STRING = u'AAAAEEEIIOOOOUUYaaaaeeeiioooouuyAaDdIiUuOoUuAaAaAaAaAaAaAaAaAaAaAaAaEeEeEeEeEeEeEeEeIiIiOoOoOoOoOoOoOoOoOoOoOoOoUuUuUuUuUuUuUuYyYyYyYy'\n",
    "\n",
    "\n",
    "def remove_accents_to_ascii(input_str):\n",
    "    string_after_filtered = ''.join(NON_ACCENT_STRING[ACCENT_STRING.index(\n",
    "        char)] if char in ACCENT_STRING else char for char in input_str)\n",
    "\n",
    "    ascii_values = [ord(character) for character in string_after_filtered]\n",
    "\n",
    "    for code in ascii_values:\n",
    "        if code >= 768 and code <= 879:\n",
    "            ascii_values.remove(code)\n",
    "\n",
    "    return ascii_values\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    string_after_filtered = ''.join(NON_ACCENT_STRING[ACCENT_STRING.index(\n",
    "        char)] if char in ACCENT_STRING else char for char in input_str)\n",
    "\n",
    "    ascii_values = [ord(character) for character in string_after_filtered]\n",
    "\n",
    "    for code in ascii_values:\n",
    "        if code >= 768 and code <= 879:\n",
    "            ascii_values.remove(code)\n",
    "\n",
    "    return ''.join(map(chr, ascii_values))\n",
    "\n",
    "def compare_ascii_string(string_1: str, string_2: str):\n",
    "    list_1 = remove_accents_to_ascii(string_1.lower())\n",
    "    list_2 = remove_accents_to_ascii(string_2.lower())\n",
    "    \n",
    "    filtered_word1 = ''.join(map(chr, list_1))\n",
    "    filtered_word2 = ''.join(map(chr, list_2))\n",
    "    \n",
    "    return filtered_word1 in filtered_word2, filtered_word1, filtered_word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trung chien rim sa te\n"
     ]
    }
   ],
   "source": [
    "print(remove_accents('Trứng Chiên Rim Sa Tế'.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(compare_ascii_string(string_1='Trứng Chiên Rim', string_2='Trứng Chiên Rim Sa Tế')[0])\n",
    "\n",
    "result, _, _ = compare_ascii_string(string_1='Trứng Chiên Rim', string_2='Trứng Chiên Rim Sa Tế')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "if 'trứng nước' in 'trứng chiên nước mắm':\n",
    "    print(True)\n",
    "else: \n",
    "    print(False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('chatbot_lib': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e53ceeece10c14de9b351bcc254f92d9bd65b66b08d1dc5918be703d9c02a900"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
